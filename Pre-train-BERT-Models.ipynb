{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook for fine-tuning BERT related models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import regex as re\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT EMBEDDER\n",
    "+ TIME OF EACH SENT\n",
    "+ Demographics? Sex / Age?\n",
    "\n",
    "LSTM (WITH ATTENTION) OVER SEQUENCE ??? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tom/phd/ADReSS_Challenge/ADReSS_Challenge'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitt_cookie_cc = '../data/Pitt/Control/cookie/**'\n",
    "pitt_fluency_cc = '../data/Pitt/Control/fluency/**'\n",
    "pitt_cookie_ad = '../data/Pitt/Dementia/cookie/**'\n",
    "pitt_fluency_ad = '../data/Pitt/Dementia/fluency/**'\n",
    "pitt_recall_ad = '../data/Pitt/Dementia/recall/**'\n",
    "pitt_sentence_ad = '../data/Pitt/Dementia/sentence/**'\n",
    "ad_datas = [pitt_cookie_ad, pitt_fluency_ad, pitt_recall_ad, pitt_sentence_ad]\n",
    "ctrl_datas = [pitt_cookie_cc, pitt_fluency_cc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_data(file_name):\n",
    "    par = {}\n",
    "    par['id'] = file_name.split('/')[-1].split('.cha')[0]\n",
    "    f = iter(open(file_name))\n",
    "    l = next(f)\n",
    "    speech = []\n",
    "    try:\n",
    "        curr_speech = ''\n",
    "        while (True):\n",
    "            if l.startswith('@ID'):\n",
    "                participant = [i.strip() for i in l.split('|')]\n",
    "                if participant[2] == 'PAR':\n",
    "                    par['mmse'] = '' if len(participant[8]) == 0 else float(participant[8])\n",
    "                    par['sex'] = participant[4][0] if len(participant[4]) else 'n/a'\n",
    "                    age = participant[3].replace(';', '')\n",
    "                    \n",
    "                    try:\n",
    "                        par['age'] = int(float(age)) if len(age) > 0 else 'n/a'\n",
    "                    except:\n",
    "                        print(participant)\n",
    "                        print(age)\n",
    "            if l.startswith('*PAR:') or l.startswith('*INV'):\n",
    "                curr_speech = l\n",
    "            elif len(curr_speech) != 0 and not(l.startswith('%') or l.startswith('*')):\n",
    "                curr_speech += l\n",
    "            elif len(curr_speech) > 0:\n",
    "                speech.append(curr_speech)\n",
    "                curr_speech = ''\n",
    "            l = next(f)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    clean_par_speech = []\n",
    "    clean_all_speech = []\n",
    "    speech_time_segments = []\n",
    "    is_par = False\n",
    "    for s in speech:\n",
    "        def _clean(s):\n",
    "            try:\n",
    "                speech_time_segments.append([*map(int, re.search('\\x15(\\d*_\\d*)\\x15', s).groups()[0].split('_'))])\n",
    "            except:\n",
    "                speech_time_segments.append([])\n",
    "            s = re.sub('\\x15\\d*_\\d*\\x15', '', s) # remove time block \n",
    "            s = re.sub('\\[.*\\]', '', s) # remove other speech artifacts [.*]\n",
    "            s = s.strip()\n",
    "            s = re.sub('\\t|\\n|<|>', '', s) # remove tab, new lines, inferred speech??, ampersand, &\n",
    "            return s\n",
    "        \n",
    "        if s.startswith('*PAR:'):\n",
    "            is_par = True\n",
    "        elif s.startswith('*INV:'):\n",
    "            is_par = False\n",
    "            s = re.sub('\\*INV:\\t', '', s) # remove prefix\n",
    "        if is_par:\n",
    "            s = re.sub('\\*PAR:\\t', '', s) # remove prefix    \n",
    "            clean_par_speech.append(_clean(s))\n",
    "        clean_all_speech.append(_clean(s))\n",
    "    \n",
    "    par['speech'] = speech\n",
    "    par['clean_speech'] = clean_all_speech\n",
    "    par['clean_par_speech'] = clean_par_speech\n",
    "    par['joined_all_speech'] = ' '.join(clean_all_speech)\n",
    "    par['joined_all_par_speech'] = ' '.join(clean_par_speech)\n",
    "    \n",
    "    # sentence times\n",
    "#     par['per_sent_times'] = [speech_time_segments[i][1] - speech_time_segments[i][0] if len(speech_time_segments[i]) else -1\n",
    "#                              for i in range(len(speech_time_segments))]\n",
    "#     par['total_time'] =  speech_time_segments[-1][1] - speech_time_segments[0][0]\n",
    "#     par['time_before_par_speech'] = speech_time_segments[0][0]\n",
    "#     par['time_between_sents'] = [0 if i == 0 else max(0, speech_time_segments[i][0] - speech_time_segments[i-1][1]) \n",
    "#                                  if len(speech_time_segments[i]) else -1\n",
    "#                                  for i in range(len(speech_time_segments))]\n",
    "    return par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(data_dir, ad=True):    \n",
    "    df = pd.DataFrame([extract_data(fn) for fn in glob(data_dir)])\n",
    "    df['ad'] = 1 if ad else 0\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df = pd.concat([parse_data(ad_dir) for ad_dir in ad_datas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_df = pd.concat([parse_data(ctrl_dir, ad=False) for ctrl_dir in ctrl_datas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([ctrl_df, ad_df]).sample(frac=1, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode out each segment into AD / control segments\n",
    "# Do not shuffle, as parent level segments have already been shuffled\n",
    "segmented_speech = df.apply(lambda r: pd.DataFrame({'id': r.id, 'speech_sent': r.clean_par_speech, 'ad': r.ad, 'mmse': r.mmse}), axis=1).tolist()\n",
    "df_segments = pd.concat(segmented_speech).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18538\n",
       "0     3156\n",
       "Name: ad, dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_segments.ad.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21694, 4)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_segments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding function\n",
    "def bert_embed(text: pd.Series, tokenizer, model):\n",
    "    tokenized = text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512)))\n",
    "\n",
    "    # pad so can be treated as one batch\n",
    "    max_len = max([len(i) for i in tokenized.values])\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "    # attention mask - zero out attention scores where there is no input to be processed (i.e. is padding)\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # check if multiple GPUs are available\n",
    "    multi_gpu = torch.cuda.device_count() > 1\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    last_hidden_states = last_hidden_states[0]\n",
    "    if device.type == 'cuda':\n",
    "        last_hidden_states = last_hidden_states.cpu()\n",
    "    features = last_hidden_states[:,0,:].numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin with Distil roBERTa\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_features = bert_embed(df_segments.speech_sent, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    let's see what's this\n",
       "1                                           well the kæl@u\n",
       "2                                            lay it down .\n",
       "3                               pants and clothes and +...\n",
       "4        &s the little boy's  the little boy's (j)ust s...\n",
       "                               ...                        \n",
       "21689                our child was taken to our hospital .\n",
       "21690    we don't know but we think this may be a very ...\n",
       "21691                                                 oh .\n",
       "21692    when I came in this office the doctor told me ...\n",
       "21693    when I came in the bedroom (.) I see the burea...\n",
       "Name: speech_sent, Length: 21694, dtype: object"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ideally fine-tuning a BERT base on these segments? Or actually ideally fine-tune on spoken speech corpus, like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speech_sent</th>\n",
       "      <th>ad</th>\n",
       "      <th>mmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173-1</td>\n",
       "      <td>let's see what's this</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173-1</td>\n",
       "      <td>well the kæl@u</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173-1</td>\n",
       "      <td>lay it down .</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173-1</td>\n",
       "      <td>pants and clothes and +...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173-1</td>\n",
       "      <td>&amp;s the little boy's  the little boy's (j)ust s...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21689</th>\n",
       "      <td>061-0</td>\n",
       "      <td>our child was taken to our hospital .</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21690</th>\n",
       "      <td>061-0</td>\n",
       "      <td>we don't know but we think this may be a very ...</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21691</th>\n",
       "      <td>061-0</td>\n",
       "      <td>oh .</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21692</th>\n",
       "      <td>061-0</td>\n",
       "      <td>when I came in this office the doctor told me ...</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21693</th>\n",
       "      <td>061-0</td>\n",
       "      <td>when I came in the bedroom (.) I see the burea...</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21694 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        speech_sent  ad mmse\n",
       "0      173-1                              let's see what's this   1    5\n",
       "1      173-1                                     well the kæl@u   1    5\n",
       "2      173-1                                      lay it down .   1    5\n",
       "3      173-1                         pants and clothes and +...   1    5\n",
       "4      173-1  &s the little boy's  the little boy's (j)ust s...   1    5\n",
       "...      ...                                                ...  ..  ...\n",
       "21689  061-0              our child was taken to our hospital .   1   26\n",
       "21690  061-0  we don't know but we think this may be a very ...   1   26\n",
       "21691  061-0                                               oh .   1   26\n",
       "21692  061-0  when I came in this office the doctor told me ...   1   26\n",
       "21693  061-0  when I came in the bedroom (.) I see the burea...   1   26\n",
       "\n",
       "[21694 rows x 4 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTAdDataset(Dataset):\n",
    "    def __init__(self, text, labels, tokenizer):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        \n",
    "        tokenized = [tokenizer.encode(x, add_special_tokens=True, max_length=512) for x in text]\n",
    "        # pad so can be treated as one batch\n",
    "        max_len = 0\n",
    "        for i in tokenized:\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "\n",
    "        padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized])\n",
    "\n",
    "        # attention mask - zero out attention scores where there is no input to be processed (i.e. is padding)\n",
    "        attention_mask = np.where(padded != 0, 1, 0)        \n",
    "        self.input_ids = padded  \n",
    "        self.attention_mask = attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return (self.input_ids[i], self.attention_mask[i]), self.labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BERTAdDataset(df.joined_all_par_speech.tolist(), df.ad.tolist(), tokenizer)\n",
    "trainloader = DataLoader(ds, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1293"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# check if multiple GPUs are available\n",
    "multi_gpu = torch.cuda.device_count() > 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.003)\n",
    "\n",
    "batch \n",
    "\n",
    "for epoch in range(5):\n",
    "    for (input_ids, attn_masks), labels in trainloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attn_masks = attn_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model.forward(input_ids, attention_mask=attn_masks)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    \n",
    "        # model eval\n",
    "        # every so many batches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:distil_bert]",
   "language": "python",
   "name": "conda-env-distil_bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
