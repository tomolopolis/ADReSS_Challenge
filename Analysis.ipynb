{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import regex as re\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, mean_squared_error\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interspeech 2020 Challenge\n",
    "Deadline: May 8th submission of predictions and paper.\n",
    "\n",
    "Main Webpage: http://www.interspeech2020.org/index.php?m=content&c=index&a=lists&catid=66\n",
    "\n",
    "Challenge Webpage: http://www.homepages.ed.ac.uk/sluzfil/ADReSS/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Plan\n",
    "Only using the transcripts:\n",
    "- [x] Simple clean and join all sentences, classifiy using DistillBERT, (BERT), (RoBERTa)   ## Done\n",
    "\n",
    "## Further Feature Engineering:\n",
    "### Time dimension\n",
    "- Embed time total time taken - parse time blocks, take first and last\n",
    "- Embed total time taken per sentence\n",
    "- Time before starting speech\n",
    "- Time in between each sentence\n",
    "- Average / min / max / median time of sentence\n",
    "- use of special characters\n",
    "- number of sentences spoken\n",
    "\n",
    "### Linguistic Features\n",
    "- Embed special character tokens in speech, pauses etc. (not sure if this needed, tokenzier / and\n",
    "- classify on a sentence level??\n",
    "- Also use the Interviewer INV, questions / speech / time...\n",
    "- Use POS Tags: as OHE vector\n",
    "\n",
    "## Demographics\n",
    "- Gender\n",
    "- Age\n",
    "\n",
    "## Fine-tuning BERT(-esque) models on spontaneous speech datasets\n",
    "- fine-tune and re-classify using other spontaneous speech datasets: \n",
    "\n",
    "### Further work on\n",
    "- Analysis of what roBERTa has actually learned in the attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ad_dir = '../train/transcription/cd/*'\n",
    "controls_dir = '../train/transcription/cc/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file_name):\n",
    "    par = {}\n",
    "    par['id'] = file_name.split('/')[-1].split('.cha')[0]\n",
    "    f = iter(open(file_name))\n",
    "    l = next(f)\n",
    "    speech = []\n",
    "    try:\n",
    "        curr_speech = ''\n",
    "        while (True):\n",
    "            if l.startswith('@ID'):\n",
    "                participant = [i.strip() for i in l.split('|')]\n",
    "                if participant[2] == 'PAR':\n",
    "                    par['mmse'] = '' if len(participant[8]) == 0 else float(participant[8])\n",
    "                    par['sex'] = participant[4][0]\n",
    "                    par['age'] = int(participant[3].replace(';', ''))\n",
    "            if l.startswith('*PAR:') or l.startswith('*INV'):\n",
    "                curr_speech = l\n",
    "            elif len(curr_speech) != 0 and not(l.startswith('%') or l.startswith('*')):\n",
    "                curr_speech += l\n",
    "            elif len(curr_speech) > 0:\n",
    "                speech.append(curr_speech)\n",
    "                curr_speech = ''\n",
    "            l = next(f)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    clean_par_speech = []\n",
    "    clean_all_speech = []\n",
    "    speech_time_segments = []\n",
    "    is_par = False\n",
    "    for s in speech:\n",
    "        def _clean(s):\n",
    "            speech_time_segments.append([*map(int, re.search('\\x15(\\d*_\\d*)\\x15', s).groups()[0].split('_'))])\n",
    "            s = re.sub('\\x15\\d*_\\d*\\x15', '', s) # remove time block \n",
    "            s = re.sub('\\[.*\\]', '', s) # remove other speech artifacts [.*]\n",
    "            s = s.strip()\n",
    "            s = re.sub('\\t|\\n|<|>', '', s) # remove tab, new lines, inferred speech??, ampersand, &\n",
    "            return s\n",
    "        \n",
    "        if s.startswith('*PAR:'):\n",
    "            is_par = True\n",
    "        elif s.startswith('*INV:'):\n",
    "            is_par = False\n",
    "            s = re.sub('\\*INV:\\t', '', s) # remove prefix\n",
    "        if is_par:\n",
    "            s = re.sub('\\*PAR:\\t', '', s) # remove prefix    \n",
    "            clean_par_speech.append(_clean(s))\n",
    "        clean_all_speech.append(_clean(s))\n",
    "    \n",
    "    par['speech'] = speech\n",
    "    par['clean_speech'] = clean_all_speech\n",
    "    par['clean_par_speech'] = clean_par_speech\n",
    "    par['joined_all_speech'] = ' '.join(clean_all_speech)\n",
    "    par['joined_all_par_speech'] = ' '.join(clean_par_speech)\n",
    "    \n",
    "    # sentence times\n",
    "    par['per_sent_times'] = speech_time_segments\n",
    "    par['total_time'] =  speech_time_segments[-1][1] - speech_time_segments[0][0]\n",
    "    par['time_before_par_speech'] = speech_time_segments[0][0]\n",
    "    par['time_between_sents'] = [0 if i == 0 else max(0, speech_time_segments[i][0] - speech_time_segments[i-1][1]) \n",
    "                                 for i in range(len(speech_time_segments))]\n",
    "    return par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_train_data():\n",
    "    return _parse_data('../data/train')\n",
    "    \n",
    "def parse_test_data():\n",
    "    return _parse_data('../data/test')\n",
    "\n",
    "def parse_pre_train_data():\n",
    "    return _parse_data('/data/train')\n",
    "\n",
    "def _parse_data(data_dir):\n",
    "    prob_ad_dir = f'{data_dir}/transcription/cd/*'\n",
    "    controls_dir = f'{data_dir}/transcription/cc/*'\n",
    "    \n",
    "    prob_ad = [extract_data(fn) for fn in glob(prob_ad_dir)]\n",
    "    controls = [extract_data(fn) for fn in glob(controls_dir)]\n",
    "    controls_df = pd.DataFrame(controls)\n",
    "    prob_ad_df = pd.DataFrame(prob_ad)\n",
    "    controls_df['ad'] = 0\n",
    "    prob_ad_df['ad'] = 1\n",
    "    df = pd.concat([controls_df, prob_ad_df]).sample(frac=1).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = parse_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>mmse</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>speech</th>\n",
       "      <th>clean_speech</th>\n",
       "      <th>all_clean_speech</th>\n",
       "      <th>per_sent_times</th>\n",
       "      <th>total_time</th>\n",
       "      <th>time_before_par_speech</th>\n",
       "      <th>time_between_sents</th>\n",
       "      <th>ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S081</td>\n",
       "      <td>19</td>\n",
       "      <td>m</td>\n",
       "      <td>69</td>\n",
       "      <td>[*PAR:\\t&lt;well the kid&gt; [//] the girl's laughin...</td>\n",
       "      <td>[well the kid  the girl's laughin(g) at her br...</td>\n",
       "      <td>well the kid  the girl's laughin(g) at her bro...</td>\n",
       "      <td>[[0, 6166], [6166, 8700], [8700, 11360], [1136...</td>\n",
       "      <td>54117</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S084</td>\n",
       "      <td>21</td>\n",
       "      <td>m</td>\n",
       "      <td>68</td>\n",
       "      <td>[*PAR:\\talright . [+ exc] \u00150_522\u0015\\n, *PAR:\\tI ...</td>\n",
       "      <td>[alright ., I see the little boy stealing cook...</td>\n",
       "      <td>alright . I see the little boy stealing cookie...</td>\n",
       "      <td>[[0, 522], [3194, 7004], [8065, 12532], [13354...</td>\n",
       "      <td>134740</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 2672, 1061, 822, 0, 0, 0, 0, 0, 0, 886, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S104</td>\n",
       "      <td>27</td>\n",
       "      <td>m</td>\n",
       "      <td>64</td>\n",
       "      <td>[*PAR:\\tokay, I see a boy in the cookie jar . ...</td>\n",
       "      <td>[okay, I see a boy in the cookie jar ., I see ...</td>\n",
       "      <td>okay, I see a boy in the cookie jar . I see he...</td>\n",
       "      <td>[[3209, 8329], [8329, 10258], [10258, 19851], ...</td>\n",
       "      <td>92350</td>\n",
       "      <td>3209</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S101</td>\n",
       "      <td>24</td>\n",
       "      <td>f</td>\n",
       "      <td>56</td>\n",
       "      <td>[*PAR:\\t(...) &amp;um &amp;t takin(g) some cookies . [...</td>\n",
       "      <td>[(...) &amp;um &amp;t takin(g) some cookies ., and &amp;f ...</td>\n",
       "      <td>(...) &amp;um &amp;t takin(g) some cookies . and &amp;f fa...</td>\n",
       "      <td>[[8671, 44188], [44188, 48558], [48558, 59393]...</td>\n",
       "      <td>214850</td>\n",
       "      <td>8671</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 13198, 1341, 1546]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S033</td>\n",
       "      <td>30</td>\n",
       "      <td>f</td>\n",
       "      <td>61</td>\n",
       "      <td>[*PAR:\\tmhm . [+ exc] \u00154672_5203\u0015\\n, *PAR:\\twe...</td>\n",
       "      <td>[mhm ., well the water's running over on the f...</td>\n",
       "      <td>mhm . well the water's running over on the flo...</td>\n",
       "      <td>[[4672, 5203], [9212, 13604], [13604, 17399], ...</td>\n",
       "      <td>42523</td>\n",
       "      <td>4672</td>\n",
       "      <td>[0, 4009, 0, 0, 0, 0, 0, 1031, 11525]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id mmse sex  age                                             speech  \\\n",
       "0  S081   19   m   69  [*PAR:\\t<well the kid> [//] the girl's laughin...   \n",
       "1  S084   21   m   68  [*PAR:\\talright . [+ exc] \u00150_522\u0015\\n, *PAR:\\tI ...   \n",
       "2  S104   27   m   64  [*PAR:\\tokay, I see a boy in the cookie jar . ...   \n",
       "3  S101   24   f   56  [*PAR:\\t(...) &um &t takin(g) some cookies . [...   \n",
       "4  S033   30   f   61  [*PAR:\\tmhm . [+ exc] \u00154672_5203\u0015\\n, *PAR:\\twe...   \n",
       "\n",
       "                                        clean_speech  \\\n",
       "0  [well the kid  the girl's laughin(g) at her br...   \n",
       "1  [alright ., I see the little boy stealing cook...   \n",
       "2  [okay, I see a boy in the cookie jar ., I see ...   \n",
       "3  [(...) &um &t takin(g) some cookies ., and &f ...   \n",
       "4  [mhm ., well the water's running over on the f...   \n",
       "\n",
       "                                    all_clean_speech  \\\n",
       "0  well the kid  the girl's laughin(g) at her bro...   \n",
       "1  alright . I see the little boy stealing cookie...   \n",
       "2  okay, I see a boy in the cookie jar . I see he...   \n",
       "3  (...) &um &t takin(g) some cookies . and &f fa...   \n",
       "4  mhm . well the water's running over on the flo...   \n",
       "\n",
       "                                      per_sent_times  total_time  \\\n",
       "0  [[0, 6166], [6166, 8700], [8700, 11360], [1136...       54117   \n",
       "1  [[0, 522], [3194, 7004], [8065, 12532], [13354...      134740   \n",
       "2  [[3209, 8329], [8329, 10258], [10258, 19851], ...       92350   \n",
       "3  [[8671, 44188], [44188, 48558], [48558, 59393]...      214850   \n",
       "4  [[4672, 5203], [9212, 13604], [13604, 17399], ...       42523   \n",
       "\n",
       "   time_before_par_speech                                 time_between_sents  \\\n",
       "0                       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1                       0  [0, 2672, 1061, 822, 0, 0, 0, 0, 0, 0, 886, 0,...   \n",
       "2                    3209  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3                    8671     [0, 0, 0, 0, 0, 0, 0, 0, 0, 13198, 1341, 1546]   \n",
       "4                    4672              [0, 4009, 0, 0, 0, 0, 0, 1031, 11525]   \n",
       "\n",
       "   ad  \n",
       "0   0  \n",
       "1   0  \n",
       "2   0  \n",
       "3   0  \n",
       "4   1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_speech = train_df.loc[:, ['clean_par_speech', 'ad', 'mmse']].apply(lambda r: pd.DataFrame({'speech_sent': r.clean_par_speech, 'ad': r.ad, 'mmse': r.mmse}), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode out each segment into AD / control segments\n",
    "pd.concat(segmented_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tran Base-line TfIdf / Random Forest Models / SVM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lines_train_df = train_df.loc[:, ['joined_all_par_speech', 'ad', 'mmse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn pipeline\n",
    "param_space = {\n",
    "    'vec__max_features': [100, 500, 1000, 10000],\n",
    "#     'vec__stop_words': ['english', None],\n",
    "#     'vec__analyzer': ['word', 'char'],\n",
    "#     'vec__sublinear_tf': [True, False],\n",
    "    'clf__n_estimators': [100, 200, 500],   # gbdt params\n",
    "#     'clf__max_depth': [3, 5, 10, 20, 50],   # gbdt params\n",
    "#     'clf__C': [0.1, 0.5, 1.],               # SVC params\n",
    "#     'clf__kernel': ['rbf', 'sigmoid']       # SVC params\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('clf', GradientBoostingClassifier())\n",
    "#     ('clf', SVC())\n",
    "])\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(train_df.joined_all_par_speech, train_df.ad, random_state=random_state, test_size=0.2)\n",
    "\n",
    "search = GridSearchCV(pipe, param_space, cv=5, n_jobs=6)\n",
    "search.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(**search.best_params_)\n",
    "pipe.fit(train_features, train_labels)\n",
    "preds = lr_clf.predict(test_features)\n",
    "print('prec, rec, f1 test', precision_recall_fscore_support(test_labels, preds))\n",
    "# print('accu', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT (type) model Experimentaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DistilBERT:\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "# For DistilroBERTa:\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.RobertaModel, ppb.RobertaTokenizer, 'distilroberta-base')\n",
    "\n",
    "# BERT Base\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# BERT Large\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-large-uncased')\n",
    "\n",
    "# roBERTa base\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.RobertaModel, ppb.RobertaTokenizer, 'roberta-base')\n",
    "\n",
    "# roBERTa large\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.RobertaModel, ppb.RobertaTokenizer, 'roberta-large')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Participant Speech\n",
    "tokenized = train_df.joined_all_par_speech.apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512)))\n",
    "# All (INV + PAR) speech\n",
    "# tokenized = train_df.joined_all_speech.apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512)))\n",
    "\n",
    "# pad so can be treated as one batch\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "# attention mask - zero out attention scores where there is no input to be processed (i.e. is padding)\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape\n",
    "\n",
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# check if multiple GPUs are available\n",
    "multi_gpu = torch.cuda.device_count() > 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = last_hidden_states[0]\n",
    "if device.type == 'cuda':\n",
    "    last_hidden_states = last_hidden_states.cpu()\n",
    "features = last_hidden_states[:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(features, train_df):\n",
    "    # AD classification task\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, train_df.ad)\n",
    "    parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "    grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "    grid_search.fit(train_features, train_labels)\n",
    "    print('best parameters:', grid_search.best_params_)\n",
    "    print('best scores: ', grid_search.best_score_)\n",
    "    lr_clf = LogisticRegression(**grid_search.best_params_)\n",
    "    lr_clf.fit(train_features, train_labels)\n",
    "    preds = lr_clf.predict(test_features)\n",
    "    print('prec, rec, f1 test', precision_recall_fscore_support(test_labels, preds))\n",
    "    def cv10_avg(score):\n",
    "        return round(cross_val_score(lr_clf, features, train_df.ad, cv=10, scoring=score).sum() / 10, 2)\n",
    "    print(f'accu:{cv10_avg(\"accuracy\")} prec:{cv10_avg(\"precision\")}, rec:{cv10_avg(\"recall\")}, f1:{cv10_avg(\"f1\")}')\n",
    "          \n",
    "    # MMSE regression task\n",
    "    # remove missing row\n",
    "    reg_features, reg_scores = np.vstack([features[0:36], features[37:]]), train_df[train_df.mmse != ''].mmse\n",
    "    train_features, test_features, train_scores, test_scores = train_test_split(reg_features, reg_scores)\n",
    "    parameters = {'alpha': np.linspace(0.001, 100, 20)}\n",
    "    grid_search = GridSearchCV(Ridge(), parameters)\n",
    "    grid_search.fit(train_features, train_scores)\n",
    "    print('best parameters:', grid_search.best_params_)\n",
    "    print('best scores: ', grid_search.best_score_)\n",
    "    reg_model = Ridge(**grid_search.best_params_)\n",
    "    reg_model.fit(train_features, train_scores)\n",
    "    preds = reg_model.predict(test_features)\n",
    "    print('rmse test:', sqrt(mean_squared_error(test_scores, preds)))\n",
    "    print('rmse cv:', cross_val_score(reg_model, reg_features, reg_scores, cv=10, scoring='neg_root_mean_squared_error').sum() / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Features\n",
    "bert_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time features\n",
    "# - Embed time total time taken \n",
    "# - parse time blocks, take first and last\n",
    "# - Embed total time taken per sentence\n",
    "# - Time before starting speech\n",
    "# - Time in between each sentence\n",
    "# - Average / min / max / median time of sentence\n",
    "time_dims = train_df.loc[:, ['total_time', 'time_before_par_speech', 'time_between_sents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_dims['avg_betweeen_sents'] = time_dims.time_between_sents.apply(lambda t: round(sum(t) / len(t)))\n",
    "# time_dims['max'] = time_dims.time_between_sents.apply(max)\n",
    "# time_dims['min'] = time_dims.time_between_sents.apply(min)\n",
    "time_dims_eng = time_dims.drop('time_between_sents', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = StandardScaler().fit_transform(time_dims_eng.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForADClassification(torch.nn.Module):\n",
    "    def __init__(self, bertModel, time_dims,):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bertModel = bertModel\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        \n",
    "        # separate Linear for OHE of time\n",
    "        bert_hidden_dim = self.bertModel\n",
    "    \n",
    "        self.classifier = nn.Linear()\n",
    "    \n",
    "    def forward(input_ids, attention_mask):\n",
    "        last_hidden_states = self.bertModel(input_ids, attention_mask=attention_mask)\n",
    "        features = last_hidden_states[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all features\n",
    "features = np.hstack([bert_features, time_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier score: 0.542 (+/- 0.23)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, train_features, train_labels)\n",
    "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:distil_bert]",
   "language": "python",
   "name": "conda-env-distil_bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
